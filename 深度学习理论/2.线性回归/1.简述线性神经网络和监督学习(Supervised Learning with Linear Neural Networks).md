
## 什么是神经网络？(What is a Linear Neural Network)

我们常常用深度学习这个术语来指训练神经网络的过程。有时它指的是特别大规模的神经网络训练。那么神经网络究竟是什么呢？

*这里我先简述关于线性回归神经网络*

举一个实际的例子： 我们希望根据房屋的面积（平方英尺）和房龄（年）来估算房屋价格（美元）。 为了开发一个能预测房价的模型，我们需要收集一个真实的数据集。 这个数据集包括了房屋的销售价格、面积和房龄。 在机器学习的术语中，该数据集称为*训练数据集*（training data set） 或*训练集*（training set）。 每行数据（比如一次房屋交易相对应的数据）称为*样本*（sample）， 也可以称为*数据点*（data point）或*数据样本*（data instance）。 我们把试图预测的目标（比如预测房屋价格）称为*标签*（label）或*目标*（target）。 预测所依据的自变量（面积和房龄）称为*特征*（feature）或*协变量*（covariate）。


让我们从一个房价预测的例子开始讲起。
###  线性模型

假设有一个数据集，它包含了六栋房子的信息。所以，你知道房屋的面积是多少平方英尺或者平方米，并且知道房屋价格。这时，想要拟合一个根据房屋面积预测房价的函数。

我们线性假设是指目标（房屋价格）可以表示为特征（面积和房龄）的加权和：

$$
\mathrm{price} = w_{\mathrm{area}} \cdot \mathrm{area} + w_{\mathrm{age}} \cdot \mathrm{age} + b.
$$
权重决定了每个特征对我们预测值的影响，因为是线性关系，我们可以得到这样的图像：
![](images/R.png)

### 激活函数

我们知道价格永远不会是负数的。因此，为了替代一条可能会让价格为负的直线，我们把直线弯曲一点，让它最终在零结束。这条粗的蓝线最终就是你的函数，用于根据房屋面积预测价格。有部分是零，而直线的部分拟合的很好。

![](images/3fe6da26014467243e3d499569be3675.png)


作为一个神经网络，这几乎可能是最简单的神经网络。我们把房屋的面积作为神经网络的输入（我们称之为$x$），通过一个节点（一个小圆圈），最终输出了价格（我们用$y$表示）。其实这个小圆圈就是一个单独的神经元。接着你的网络实现了左边这个函数的功能。

在有关神经网络的文献中，经常看得到这个函数。从趋近于零开始，然后变成一条直线。这个函数被称作**ReLU**激活函数，它的全称是**Rectified Linear Unit**。rectify（修正）可以理解成$max(0,x)$，这也是你得到一个这种形状的函数的原因。

### 损失函数

开始寻找最好的*模型参数*（model parameters）$\mathbf{w}$和$b$之前， 我们还需要两个东西： 
- 一种模型质量的度量方式； 
- 一种能够更新模型以提高模型预测质量的方法

_损失函数_（loss function）能够量化目标的实际值与预测值之间的差距。 通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。 回归问题中最常用的损失函数是平方误差函数。

当样本$i$的预测值为$\hat{y}^{(i)}$，其相应的真实标签为时$y^{(i)}$,损失函数计算公式：
$$l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2.$$
常数$\frac{1}{2}$不会带来本质的差别，但这样在形式上稍微简单一些 （因为当我们对损失函数求导后常数系数为1）。 由于训练数据集并不受我们控制，所以经验误差只是关于模型参数的函数。 为了进一步说明，来看下面的例子。 我们为一维情况下的回归问题绘制图像


![](images/1.png)

我们需要计算n个数据的损失函数，也就是求和，所以我们便有：
$$L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.$$

### 优化模型

#### 随机梯度下降

（这里随便举一个例子，随机梯度下降最为简单，优化模型在后面会单独列出一章）

我们得到损失函数，这时候想办法要把损失函数值降下去，意味着调整预测值接近真实数据值，也就是在这个函数这里，我们需要调整b值，使得损失函数最小，那么怎么调整预测值呢？

不难看出，损失函数里面将值全部带入是一个关于$b$的2次函数
![](images/Snipaste_2024-04-13_11-16-02.png)


这里选择*梯度下降*（gradient descent）的方法， 这种方法几乎可以优化所有深度学习模型。 它通过不断地在损失函数递减的方向上更新参数来降低误差。

梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值） 关于模型参数的导数（在这里也可以称为梯度）。每一次减去此处的导数，当模型到达最低点，导数也就是0，当你的数据没有变化的时候，在函数图像上也就是在损失函数最低的时候。

![](images/Snipaste_2024-04-13_11-18-45.png)

数学公式表示这一更新过程：
$$(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b).$$

![](images/4fb3b91114ecb2cd81ec9f3662434d81.jpg)

算法的步骤如下： 
- 初始化模型参数的值，如随机初始化； 
- 从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤。 对于平方损失和仿射变换，我们可以明确地写成如下形式

$$\begin{split}\begin{aligned} \mathbf{w} &\leftarrow \mathbf{w} -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b) = \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right),\\ b &\leftarrow b -  \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_b l^{(i)}(\mathbf{w}, b)  = b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right). \end{aligned}\end{split}$$

但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。 因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本， 这种变体叫做*小批量随机梯度下降*（minibatch stochastic gradient descent）。


线性回归恰好是一个在整个域中只有一个最小值的学习问题。 但是对像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。 深度学习实践者很少会去花费大力气寻找这样一组参数，使得在_训练集_上的损失达到最小。 事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失， 这一挑战被称为**泛化**（generalization）。



---

以上便是一个线性神经网络的全部过程。这里包括选择数学公式，预测结果，模拟函数，激活函数，带入结果，计算损失函数，调整函数更好拟合数据，导入数据预测未来。


---


## 线性回归到深度网络


但是一般深度学习不会是单个或是两个因素去决定事情，比如说房价的因素可能还会有以下这些：
![](images/d900d26b850d55abce36d4c8daf71327.png)

![](images/7a0e0d40f4ba80a0466f0bd7aa9f8537.png)

深度学习从业者喜欢绘制图表来可视化模型中正在发生的事情。
因此输入层中的*输入数*（或称为*特征维度*，feature dimensionality）为$d$。 网络的输出为$o_1$，因此输出层中的*输出数*是1。 需要注意的是，输入值都是已经给定的，并且只有一个*计算*神经元。 由于模型重点在发生计算的地方，所以通常我们在计算层数时不考虑输入层。 也就是说， 神经网络的层数为1。 我们可以将线性回归模型视为仅由单个人工神经元组成的神经网络，或称为单层神经网络。

![](images/Snipaste_2024-04-13_13-45-17.png)




## 神经网络的监督学习(Supervised Learning with Neural Networks)

监督学习中核心在于使用带有标签的训练数据来教导模型如何预测未知数据的输出。在这种学习模式下，每一个输入样本都有一个与之对应的正确输出（标签），模型的任务是通过学习这些样本来预测新样本的输出。

关于神经网络也有很多的种类，考虑到它们的使用效果，有些使用起来恰到好处，但事实表明，到目前几乎所有由神经网络创造的经济价值，本质上都离不开一种叫做监督学习的机器学习类别，让我们举例看看。

在监督学习中你有一些输入$x$，你想学习到一个函数来映射到一些输出$y$，比如我们之前提到的房价预测的例子，你只要输入有关房屋的一些特征，试着去输出或者估计价格$y$。我们举一些其它的例子，来说明神经网络已经被高效应用到其它地方。

![](/images/ec9f15da25c4072eeedc9ba7fa363f80.png)

那么深度学习系统已经可以创造如此多的价值，通过智能的选择，哪些作为$x$哪些作为$y$，来针对于你当前的问题，然后拟合监督学习部分，往往是一个更大的系统，比如自动驾驶。这表明神经网络类型的轻微不同，也可以产生不同的应用，比如说，应用到我们在上一个视频提到的房地产领域，我们不就使用了一个普遍标准神经网络架构吗？

也许对于房地产和在线广告来说可能是相对的标准一些的神经网络，正如我们之前见到的。对于图像应用，我们经常在神经网络上使用卷积（**Convolutional Neural Network**），通常缩写为**CNN**。对于序列数据，例如音频，有一个时间组件，随着时间的推移，音频被播放出来，所以音频是最自然的表现。作为一维时间序列（两种英文说法**one-dimensional time series / temporal sequence**）.对于序列数据，经常使用**RNN**，一种递归神经网络（**Recurrent Neural Network**），语言，英语和汉语字母表或单词都是逐个出现的，所以语言也是最自然的序列数据，因此更复杂的**RNNs**版本经常用于这些应用。

对于更复杂的应用比如自动驾驶，有一张图片，可能会显示更多的**CNN**卷积神经网络结构，其中的雷达信息是完全不同的，可能会有一个更定制的，或者一些更复杂的混合的神经网络结构。所以为了更具体地说明什么是标准的**CNN**和**RNN**结构，在文献中可能见过这样的图片，这是一个标准的神经网络。

![](/images/6cda3361ce142b4347593db842d95ef0.png)

也可能见过这样的图片，这是一个卷积神经网络的例子。

![](images/1bebe0ac41715ef8132f2d802968495c.png)

卷积网络(**CNN**)通常用于图像数据。

可能也会看到这样的图片.

![](images/4656617e30e7ad44490fe605b2e49e56.png)

递归神经网络(**RNN**)非常适合这种一维序列，数据可能是一个时间组成部分。

可能也听说过机器学习对于结构化数据和非结构化数据的应用，结构化数据意味着数据的基本数据库。例如在房价预测中，可能有一个数据库，有专门的几列数据告诉你卧室的大小和数量，这就是结构化数据。
意思是每个特征，比如说房屋大小卧室数量都有一个很好的定义。

相反非结构化数据是指比如音频，原始音频或者你想要识别的图像或文本中的内容。这里的特征可能是图像中的像素值或文本中的单个单词。

![](images/86a39d40cb13842cd6c06463cd9b4a83.png)

从历史经验上看，处理非结构化数据是很难的，与结构化数据比较，让计算机理解非结构化数据很难，而人类进化得非常善于理解音频信号和图像，文本是一个更近代的发明，但是人们真的很擅长解读非结构化数据。

但结果也表明，神经网络在许多短期经济价值的创造，也是基于结构化数据的。比如更好的广告系统、更好的利润建议，还有更好的处理大数据的能力。许多公司不得不根据神经网络做出准确的预测。


