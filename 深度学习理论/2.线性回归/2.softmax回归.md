## 分类回归介绍

回归可以用于预测多少的问题。 比如预测房屋被售出价格，或者棒球队可能获得的胜场数，又或者患者住院的天数。

事实上，我们也对*分类*问题感兴趣：不是问“多少”，而是问“哪一个”：

- 某个电子邮件是否属于垃圾邮件文件夹？
    
- 某个用户可能注册或不注册订阅服务？
    
- 某个图像描绘的是驴、狗、猫、还是鸡？
    
- 某人接下来最有可能看哪部电影？

Softmax 回归，通常称为多类逻辑回归或多项逻辑回归，是逻辑回归的一种扩展，使其可以处理多类分类问题。在基本的逻辑回归中，模型被用来预测两个类别（二分类）之间的概率。而 Softmax 回归允许模型预测多于两个类别的情况，这使得它在多类分类问题中非常有用，例如手写数字识别、物体分类等。

## Softmax 函数

Softmax 回归的核心是 Softmax 函数。它将一个包含任意实数的向量转换成另一个同样长度的实数向量，但转换后的向量中的元素值域为 [0, 1] 且总和为 1，使得它们可以被解释为概率。公式如下：
$$\operatorname{Softmax}\left(z_i\right)=\frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}$$

这里，$z_i$​ 是未归一化的对数概率（也称为 logits），代表属于某类的相对证据，而分母是所有类别 logits 的指数和，用于归一化，确保输出概率总和为 1。$K$ 是类别的总数。


 softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持可导的性质。 为了完成这一目标，我们首先对每个未规范化的预测求幂，这样可以确保输出非负。 为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。一般写法是：
$$\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o})\quad \text{其中}\quad \hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}$$

在 Softmax 回归中，每个类 $k$ 都有一组参数（权重和偏置），模型会输出一个 logits 向量 $z$，其维度等于类别数 $K$。输入特征 $x$ 通过以下线性方程计算 logits：

$$z={W}x+b$$
这里$W$是权重，$b$是偏置向量。


### 网络架构

为了估计所有可能类别的条件概率，我们需要一个有多个输出的模型，每个类别对应一个输出。 为了解决线性模型的分类问题，我们需要和输出一样多的仿射函数（affine function）。 每个输出对应于它自己的仿射函数。 
在我们的例子中，由于我们有4个特征和3个可能的输出类别， 我们将需要12个标量来表示权重（带下标的$W$）， 3个标量来表示偏置（带下标的$b$）。
下面我们为每个输入计算三个未规范化的预测（logit）:$o_1,o_2,o_3$
$$\begin{split}\begin{aligned}
o_1 &= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\\
o_2 &= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,\\
o_3 &= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3.
\end{aligned}\end{split}$$

softmax回归的输出仍然由输入特征的仿射变换决定。 因此，softmax回归是一个*线性模型（linear model）*。

我们可以用神经网络图来描述这个计算过程。 与线性回归一样，softmax回归也是一个单层神经网络。 由于计算每个输出$o_1,o_2,o_3$取决于所有输入$x_1、x_2、x_3、x_4$ ,所以softmax回归的输出层也是全连接层.
![](Snipaste_2024-04-13_15-03-23.png) 

所以这里便解释清楚了softmax回归的矢量计算表达式为：
$$\begin{split}\begin{aligned} \mathbf{O} &= \mathbf{X} \mathbf{W} + \mathbf{b}, \\ \hat{\mathbf{Y}} & = \mathrm{softmax}(\mathbf{O}). \end{aligned}\end{split}$$


## 损失函数

我们需要一个损失函数来度量预测的效果。 我们将使用最大似然估计，这与在线性回归中的方法相同。

### 对数似然

假设整个数据集$\{\mathbf{X}, \mathbf{Y}\}$具有$n$个样本， 其中索引$i$的样本由特征向量$\mathbf{x}^{(i)}$和独热标签向量$\mathbf{y}^{(i)}$组成。 我们可以将估计值与实际值进行比较：
$$P(\mathbf{Y} \mid \mathbf{X}) = \prod_{i=1}^n P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}).$$
一般我们将相乘的式子转换成指数就可以做到相加,且我们最大化$P(\mathbf{Y} \mid \mathbf{X})$，相当于最小化负对数似然：：
$$-\log P(\mathbf{Y} \mid \mathbf{X}) = \sum_{i=1}^n -\log P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})
= \sum_{i=1}^n l(\mathbf{y}^{(i)}, \hat{\mathbf{y}}^{(i)}),$$
这时候算损失函数：
$$l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j.$$


在最初我们推出来的softmax函数，可以将函数带入这里的损失函数里面：
$$\begin{split}\begin{aligned}
\hat{y_i} = \frac{\exp({o_j})} {\sum_{k=1}^q \exp(o_k)}
\end{aligned}\end{split}

\begin{split}\begin{aligned}
l(\mathbf{y}, \hat{\mathbf{y}}) &=  - \sum_{j=1}^q y_j \log \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} \\
&= \sum_{j=1}^q y_j \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j\\
&= \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j.
\end{aligned}\end{split}$$
相对于任何未规范化的预测$o_j$的导数，我们得到:
$$\partial_{o_j} l(\mathbf{y}, \hat{\mathbf{y}}) = \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} - y_j = \mathrm{softmax}(\mathbf{o})_j - y_j.$$

